from pydantic import BaseModel, Field 
from typing import Optional
from loguru import logger 
import json 
import os 

class Transformation(BaseModel):
    description: str = Field(description="Description of the transformation")
    difficulty_level: float = Field(description="Difficulty level of the transformation from 0 to 1 where 0 means it make the problem easy and 1 means it makes it super hard")

class SubProblem(BaseModel):
    """ Step3 output schema
        we generate list of N sub problems per problem during step3 to make it easier (where N is fixed for now but later 
        should be generated by difficulty Engine)
    """
    original_question : Optional[str] = Field(description="Original question before any transformations applied", default=None)
    transformations_applid : Optional[list[Transformation]] = Field(description="List of transformations applied to the problem", default=[])
    sub_question: str = Field(description="Sub question, which we hope to be solvable by small LLM")
    difficulty_level: Optional[float] = Field(description="Difficulty level of the sub problem from 0 to 1 where 0 is easy and 1 is super hard", default=None)
    answer: Optional[str] = Field(description="Correct answer for the problem", default=None)

class Problem(BaseModel):
    """Problem Schema"""
    question : str = Field(description="Main question, which we hope to be non solvable by small LLM, but could be solvable by larger LLM")
    answer: str  = Field(description="Main correct answer for the problem, should be generated by larger LLM or manually", default=None)
    sub_problems: list[SubProblem] =  Field(description="List of sub problems generated at step3 to make it easier", default=[])
    difficulty_level: Optional[float] = Field(description="Difficulty level of the problem, from 0 to 1 where 0 is easy and 1 is super hard ", default=0)
    is_solvable: bool = Field(description="Whether the problem is solvable or not by small llm to be tuned", default=False)


class Dataset(BaseModel):
    """Dataset Schema"""
    problems: list[Problem]
    description: Optional[str] = None 
    model_intelligence_ratio: Optional[float] = Field(description="Decide how intelligent the model is. The Difficulty Threshold after which LLM cant solve the problem", default=0)
    metadata: Optional[dict] = Field(description="any extra metadata or descriptions you want to store about the dataset", default={})
    @staticmethod
    def from_json(json_path: str) -> "Dataset":
        """ load dataset from json """
        with open(json_path, "r") as f:
            data = json.load(f)
        return Dataset.model_validate(data)
   
    def to_json(self, export_path:str) -> None:
        """ export dataset to json """

        if os.path.exists(export_path):
            logger.warning(f"Dataset already exists at {export_path}. Skipping dataset generation")
            return 
        json_str = json.dumps(self.model_dump(), indent=4)
        with open(export_path, "w") as f:
            f.write(json_str)
        logger.success(f"Dataset exported successfully at {export_path}")